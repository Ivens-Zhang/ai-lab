{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "    \n",
    "# 人工智能高阶人才培训班\n",
    "\n",
    "<br>\n",
    "\n",
    "> ## word2vec_RNN\n",
    "</center>\n",
    "<img src='../static/img/logo.jpg' align='right' style=\"width:260px;height:60px;display:block\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Picture/rnn.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 导入工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.9/site-packages (4.0.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.9/site-packages (from gensim) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.9/site-packages (from gensim) (1.20.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.9/site-packages (from gensim) (1.6.3)\n",
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.2 MB 42 kB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314477 sha256=37bf14e06d90f71d1721040b237d07def9220911f12dd6d02bf322db33899dee\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/7d/74/cf/08c94db4b784e2c1ef675a600b7b5b281fd25240dcb954ee7e\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "!pip install gensim\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TimeDistributed,Input,Conv2D,MaxPool2D,SpatialDropout1D,concatenate,Flatten,Dense,Dropout,Embedding,SimpleRNN,Reshape,GRU,LSTM\n",
    "from tensorflow.keras import Sequential,optimizers,losses\n",
    "from tensorflow.keras.models import Model,model_from_yaml\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import multiprocessing\n",
    "import yaml\n",
    "!pip install jieba\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.数据存储地址，及参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_path = '../../data/day10-nlp-data/chineseStopWords.txt'\n",
    "neg_data_path = '../../data/day10-nlp-data/film_review/neg.xlsx'\n",
    "pos_data_path = '../../data/day10-nlp-data/film_review/pos.xlsx'\n",
    "\n",
    "vocab_dim = 60\n",
    "maxlen = 50\n",
    "n_iterations = 1  \n",
    "n_exposures = 10\n",
    "window_size = 7\n",
    "batch_size = 100\n",
    "n_epoch = 10\n",
    "input_length = 50\n",
    "cpu_count = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/day10-nlp-data/film_review/neg.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7f1751109de7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mneg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_data_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_data_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 )\n\u001b[1;32m    298\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1072\u001b[0m                     \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m                 )\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(path, content, storage_options)\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mcontent_or_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     ) as handle:\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/day10-nlp-data/film_review/neg.xlsx'"
     ]
    }
   ],
   "source": [
    "neg=pd.read_excel(neg_data_path,index_col=None,header = None)\n",
    "pos=pd.read_excel(pos_data_path,index_col=None,header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 查看消极评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.2查看积极评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.数据连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment=np.concatenate((pos[0], neg[0]))\n",
    "y = np.concatenate((np.ones(len(pos),dtype=int), np.zeros(len(neg),dtype=int)))\n",
    "comment = comment.astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinese_word_cut(text):\n",
    "    \"\"\"\n",
    "    jieba分词\n",
    "    \"\"\"\n",
    "    text = [\" \".join(jieba.cut(document)) for document in text]\n",
    "    return text\n",
    "comment =chinese_word_cut(comment)\n",
    "comment[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.去停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopword_list():\n",
    "    \"\"\"\n",
    "    构建停用词列表\n",
    "    \"\"\"\n",
    "    stopword_list = [sw.replace('\\n', '') for sw in open(stop_word_path, encoding='gb18030').readlines()]\n",
    "    return stopword_list\n",
    "stopword_list = get_stopword_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    去停用词\n",
    "    \"\"\"\n",
    "    mytext = ''\n",
    "    text= text.split()\n",
    "    for i,j in enumerate(text):  \n",
    "        if j not in stopword_list:\n",
    "            mytext+=j\n",
    "            if i!=len(text)-1:\n",
    "                mytext+=' '\n",
    "    return mytext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = [remove_stopwords(text) for text in comment]\n",
    "comment[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.构建词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建词典映射\n",
    "def create_dictionaries(model=None,comment=None):\n",
    "    \"\"\"\n",
    "    建立词语-向量映射表,\n",
    "    model:词向量模型\n",
    "    comment:预处理后的数据集\n",
    "    \n",
    "    返回值：\n",
    "    w2indx 词典（格式为：索引+词语）\n",
    "    w2vec  词语对应的向量\n",
    "    comment  向量化后的数据集（词语-索引序号）\n",
    "    \n",
    "    \"\"\"\n",
    "    if (comment is not None) and (model is not None):\n",
    "        gensim_dict = Dictionary()\n",
    "        gensim_dict.doc2bow(model.wv.vocab.keys(),\n",
    "                            allow_update=True)\n",
    "        w2indx = {v: k+1 for k, v in gensim_dict.items()}#所有频数超过10的词语的索引\n",
    "        w2vec = {word: model[word] for word in w2indx.keys()}#所有频数超过10的词语的词向量\n",
    "\n",
    "\n",
    "        def parse_dataset(comment):\n",
    "            ''' \n",
    "               Words become integers\n",
    "            '''\n",
    "            data=[]\n",
    "            for sentence in comment:\n",
    "                new_txt = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        new_txt.append(w2indx[word])\n",
    "                    except:\n",
    "                        new_txt.append(0)\n",
    "                data.append(new_txt)\n",
    "            return data\n",
    "        comment=parse_dataset(comment)\n",
    "        #每个句子所含词语对应的索引，所以句子中含有频数小于10的词语，索引为0；maxlen，每个句子最大长度\n",
    "        comment= sequence.pad_sequences(comment, maxlen=maxlen)\n",
    "        return w2indx, w2vec,comment\n",
    "    else:\n",
    "        print('No data provided...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.加载word2vec模型\n",
    "\"\"\"\n",
    "model = Word2Vec.load('model/Word60.model') \n",
    "\n",
    "\"\"\"\n",
    "2.创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\n",
    "\"\"\"\n",
    "index_dict, word_vectors,comment = create_dictionaries(model=model,comment=comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.n_symbols:所有单词的索引数，频数小于10的词语索引为0 ,所以加1\n",
    "\"\"\"\n",
    "n_symbols = len(index_dict) + 1 \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "2. embedding_weights:索引为0的词语（频数小于10），词向量全为0\n",
    "\"\"\"\n",
    "embedding_weights = np.zeros((n_symbols, vocab_dim))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "3.从索引为1的词语开始，对每个词语对应其词向量\n",
    "\"\"\"\n",
    "for word, index in index_dict.items():\n",
    "    embedding_weights[index, :] = word_vectors[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_test_split函数用于将矩阵随机划分为训练子集和测试子集，并返回划分好的训练集测试集样本和训练集测试集标签。\n",
    "\"\"\"\n",
    "x_train, x_test, y_train, y_test = train_test_split(comment, y, test_size=0.2)\n",
    "print(x_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.RNN创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.定义RNN网络，\n",
    "    Embedding层:\n",
    "        input_dim：大或等于0的整数，字典长度，即输入数据最大下标+1\n",
    "        output_dim：大于0的整数，代表全连接嵌入的维度\n",
    "        input_length：当输入序列的长度固定时，该值为其长度。如果要在该层后接Flatten层，然后接Dense层，则必须指定该参数，\n",
    "                      否则Dense层的输出维度无法自动推断。\n",
    "        weights：可以通过weights参数指定初始的weights参数，weights是一个列表\n",
    "    因为Embedding层是不可导的，所以把embedding放在中间层是没有意义的,emebedding只能作为第一层\n",
    "    \n",
    "    SimpleRNN层：RNN在Keras中对应SimpleRNN层，\n",
    "        activation:激活函数\n",
    "        units:神经元个数\n",
    "    Dropout层：用于防止过拟合，参数可以调整\n",
    "    Dense层：\n",
    "        units: 该层的神经单元结点数。 \n",
    "        activation: 激活函数.          \n",
    "\"\"\"\n",
    "model = Sequential([Embedding(input_dim=n_symbols,output_dim=vocab_dim,\n",
    "                        weights=[embedding_weights],input_length=input_length),\n",
    "                    SimpleRNN(100,activation='relu'),\n",
    "                    Dropout(0.5),\n",
    "                    Dense(25,activation='relu'),\n",
    "                    Dense(1,activation='sigmoid')])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "2.compile模型训练的BP模式设置：  \n",
    "    loss： 字符串（预定义损失函数名）或损失函数\n",
    "    optimizer： 字符串（预定义优化器名）或优化器对象\n",
    "    metrics： 列表，包含评估模型在训练和测试时的网络性能的指标，典型用法是metrics=[‘accuracy’]\n",
    "\n",
    "\"\"\"\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "3.model.summary()输出模型各层的参数状况\n",
    "\"\"\"\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 网络结构\n",
    "LSTM_model =Sequential([\n",
    "        Embedding(input_dim=n_symbols,output_dim=vocab_dim,\n",
    "                        weights=[embedding_weights],\n",
    "                        input_length=input_length), \n",
    "LSTM(100,activation='relu'),\n",
    "Dropout(0.5),\n",
    "Dense(25, activation='relu'),\n",
    "Dense(1, activation='sigmoid')])\n",
    "LSTM_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "LSTM_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model.fit:\n",
    "fit函数返回一个History的对象，其History.history属性记录了损失函数和其他指标的数值随epoch变化的情况，如果有验证集的话，也包含了验证集的这些指标变化情况\n",
    "    x：输入数据。如果模型只有一个输入，那么x的类型是numpy array，如果模型有多个输入，那么x的类型应当为list，list的元素是对应于各个输入的numpy array\n",
    "    y：标签，numpy array\n",
    "    batch_size：整数，指定进行梯度下降时每个batch包含的样本数。训练时一个batch的样本会被计算一次梯度下降，使目标函数优化一步。\n",
    "    epochs：整数，训练终止时的epoch值，训练将在达到该epoch值时停止，当没有设置initial_epoch时，它就是训练的总轮数，否则训练的总轮数为epochs - inital_epoch\n",
    "    verbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录\n",
    "    shuffle：布尔值或字符串，一般为布尔值，表示是否在训练过程中随机打乱输入样本的顺序。若为字符串“batch”，则是用来处理HDF5数据的特殊情况，它将在batch内部将数据打乱。\n",
    "\n",
    "\"\"\"\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epoch, validation_split=0.2,shuffle=True,callbacks=[reduce_lr,early_stopping,model_checkpoint])\n",
    "\n",
    "\n",
    "\n",
    "y_pre = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce_lr_lstm = ReduceLROnPlateau(monitor='val_loss', patience=10, mode='auto')\n",
    "# early_stopping_lstm = EarlyStopping(monitor='val_loss', patience=5)\n",
    "# model_checkpoint_lstm = ModelCheckpoint('./model/LSTM/bestmodel/model_{epoch:02d}-{val_accuracy:.2f}.hdf5', save_best_only=True, save_weights_only=True)\n",
    "# lstm_history = LSTM_model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epoch, validation_split=0.2,shuffle=True,callbacks=[reduce_lr_lstm,early_stopping_lstm,model_checkpoint_lstm])\n",
    "# #验证\n",
    "# lstm_y_pre = LSTM_model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. 评估与保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "评估模型\n",
    " model.evaluate（）返回的是 损失值和你选定的指标值\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "scores = model.evaluate(x_test, y_test,verbose=0)\n",
    "print('test_loss: %f, accuracy: %f' % (scores[0], scores[1]))\n",
    "\n",
    "# lstm_scores = LSTM_model.evaluate(x_test, y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "保存模型\n",
    "  1. model.to_yaml() 以yaml格式保存模型结果（不含模型权重）\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "yaml_string = model.to_yaml()\n",
    "with open('./model/RNN/comment_bestmodel/rnn.yml', 'w') as outfile:\n",
    "    outfile.write( yaml.dump(yaml_string, default_flow_style=True))\n",
    "\n",
    "\"\"\"\n",
    "  2.save_weights()保存的模型结果，它只保存了模型的参数，但并没有保存模型的图结构\n",
    "\"\"\"    \n",
    "\n",
    "model.save_weights('./model/RNN/comment_bestmodel/rnn.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yaml_string1 = LSTM_model.to_yaml()\n",
    "# with open('./model/LSTM/comment_bestmodel/lstm.yml', 'w') as outfile:\n",
    "#     outfile.write( yaml.dump(yaml_string1, default_flow_style=True))\n",
    "# LSTM_model.save_weights('./model/LSTM/comment_bestmodel/lstm.h5')\n",
    "# print('test_loss: %f, accuracy: %f' % (lstm_scores[0], lstm_scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.检验模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './model/RNN/comment_bestmodel/rnn.yml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e1ca70cedd9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0m加载模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/RNN/comment_bestmodel/rnn.yml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0myaml_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_yaml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myaml_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model/RNN/comment_bestmodel/rnn.yml'"
     ]
    }
   ],
   "source": [
    "string=['非常感动，非常好看','不太好看','看完之后很感动','一般般吧']\n",
    "\n",
    "\"\"\"\n",
    "  加载模型\n",
    "\"\"\"\n",
    "with open('./model/RNN/comment_bestmodel/rnn.yml', 'r') as f:\n",
    "    yaml_string = yaml.load(f)\n",
    "model = model_from_yaml(yaml_string)\n",
    "model.load_weights('./model/RNN/comment_bestmodel/rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  预处理\n",
    "\"\"\"\n",
    "test_string=chinese_word_cut(string)\n",
    "test_string = [remove_stopwords(text) for text in test_string]\n",
    "test_string=np.array(test_string)\n",
    "test_string.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   word2vec模型加载\n",
    "\"\"\"\n",
    "w2v_model=Word2Vec.load('model/Word60.model')\n",
    "_,_,test_string=create_dictionaries(w2v_model,test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  model.predict_classes预测的是类别，打印出来的值就是类别号\n",
    "\"\"\"\n",
    "result=model.predict_classes(test_string)\n",
    "for i in range(len(string)):\n",
    "    print(string[i],\":\",result[i][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
